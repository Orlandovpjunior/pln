{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YPBVOwcm-xQ"
      },
      "source": [
        "# **Dados:**\n",
        "\n",
        " **Aluno(a):** `Adicione seu nome aqui`\n",
        "\n",
        " **Matrícula:** `Adicione sua matrícula aqui`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzRN0mmg27MZ"
      },
      "source": [
        "# Objetivo:\n",
        "\n",
        "Nesse laboratório, você extrairá um romance de um arquivo HTML retirado do site Projeto Gutenberg (que contém um grande corpus de livros) usando o pacote de requisições do Python. Em seguida, você normalizará e tokenizará o texto. Você também irá analisar a distribuição de palavras. Ao final você deverá simular a execução do algoritmo BPE (Byte-Pair Encoding) e calcular a distância de edição entre duas palavras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEgDOZs7qkRe"
      },
      "source": [
        "# **Bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "D9tkHpFtqmTD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/orlandojunior/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /home/orlandojunior/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czDyUObFndhI"
      },
      "source": [
        "# **Obtenção dos dados**\n",
        "\n",
        "Para a atividade desse laborátorio, iremos utilizar dados textuais oriundos do **[Project Gutenberg](https://www.gutenberg.org/)**. O texto escolhido é a obra Moby Dick."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ba6eYoitlP77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            "<head>\n",
            "  <meta charset=\"utf-8\">\n",
            "<title>Moby Dick; or The Whale</title>\n",
            "\n",
            "<style>\n",
            "\n",
            "    body {margin-left:10%; margin-right:10%; text-align:justify }\n",
            "    p { text-indent: 1em; margin-top: .25em; margin-bottom: .25em; }\n",
            "    H1,H2,H3,H4,H5,H6 { text-align: center; margin-left: 15%; margin-right: 15%; }\n",
            "    hr  { width: 50%; text-align: center;}\n",
            "    blockquote {font-size: 100%; margin-left: 0%; margin-right: 0%;}\n",
            "    .mynote    {background-color: #DDE; color: #000; padding: .5em; margin-left: 10%; margin-right: 10%; font-family: sans-serif; font-size: 95%;}\n",
            "    .toc       { margin-left: 10%; margin-bottom: .75em;}\n",
            "    pre        { font-family: times new roman; font-size: 100%; margin-left: 10%;}\n",
            "\n",
            "    table      {margin-left: 10%;}\n",
            "\n",
            "a:link {color:blue;\n",
            "\t\ttext-decoration:none}\n",
            "link {color:blue;\n",
            "\t\ttext-decoration:none}\n",
            "a:visited {color:blue;\n",
            "\t\ttext-decoration:none}\n",
            "a:hover {color:red}\n",
            "\n",
            "</style>\n",
            "  </head>\n",
            "  <body>\n",
            "<div>*** START OF THE PROJECT GUTENBERG EBOOK 2701 ***</div>\n",
            "    <h1>\n",
            "      MOBY-DICK;<br><br>or, THE WHALE.\n",
            "    </h1>\n",
            "    <p>\n",
            "      <br>\n",
            "    </p>\n",
            "    <h2>\n",
            "      By Herman Melville\n",
            "    </h2>\n",
            "    <p>\n",
            "      <br> <br>\n",
            "    </p>\n",
            "    <hr>\n",
            "    <p>\n",
            "      <br> <br>\n",
            "    </p>\n",
            "    <blockquote>\n",
            "      <p class=\"toc\" style=\"font-size: x-large;\">\n",
            "        <b>CONTENTS</b>\n",
            "      </p>\n",
            "      <p>\n",
            "        <br>\n",
            "      </p>\n",
            "      <p class=\"toc\">\n",
            "        <a href=\"#link2H_4_0002\"> ETYMOLOGY. </a>\n",
            "      </p>\n",
            "      <p class=\"toc\">\n",
            "        <a href=\"#link2H_4_0003\"> EXTRACTS (Supplied by a Sub-Sub-Librarian).\n",
            "        </a>\n",
            "      </p>\n",
            "      <p>\n",
            "        <br>\n",
            "      </p>\n",
            "      <p class=\"toc\">\n",
            "        <a href=\"#link2HCH0001\"> CHAPTER 1. Loomings. </a>\n",
            "      </p>\n",
            "      <p class=\"toc\">\n",
            "        <a href=\"#link2HCH0002\"> CHAPTER 2. The Carpet-Bag. </a>\n",
            "      </p>\n",
            "      <p class=\"toc\">\n",
            "        <a href=\"#link2HCH0003\"> CHAPTER 3. The Spouter-Inn. </a>\n",
            "      </p>\n",
            "      <p class=\"toc\">\n",
            "        <a href=\"#lin\n"
          ]
        }
      ],
      "source": [
        "#Endereço do texto\n",
        "r = requests.get(\"https://www.gutenberg.org/files/2701/2701-h/2701-h.htm\")\n",
        "\n",
        "r.encoding = 'utf-8'\n",
        "\n",
        "html = r.text\n",
        "\n",
        "print(html[0:2000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wb9wO9ufpCx5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h her surf.\n",
            "      Right and left, the streets take you waterward. Its extreme downtown is\n",
            "      the battery, where that noble mole is washed by waves, and cooled by\n",
            "      breezes, which a few hours previous were out of sight of land. Look at the\n",
            "      crowds of water-gazers there.\n",
            "    \n",
            "\n",
            "      Circumambulate the city of a dreamy Sabbath afternoon. Go from Corlears\n",
            "      Hook to Coenties Slip, and from thence, by Whitehall, northward. What do\n",
            "      you see?—Posted like silent sentinels all around the town, stand\n",
            "      thousands upon thousands of mortal men fixed in ocean reveries. Some\n",
            "      leaning against the spiles; some seated upon the pier-heads; some looking\n",
            "      over the bulwarks of ships from China; some high aloft in the rigging, as\n",
            "      if striving to get a still better seaward peep. But these are all\n",
            "      landsmen; of week days pent up in lath and plaster—tied to counters,\n",
            "      nailed to benches, clinched to desks. How then is this? Are the green\n",
            "      fields gone? What do they here?\n",
            "    \n",
            "\n",
            "      But look! here come more crowds, pacing straight for the water, and\n",
            "      seemingly bound for a dive. Strange! Nothing will content them but the\n",
            "      extremest limit of the land; loitering under the shady lee of yonder\n",
            "      warehouses will not suffice. No. They must get just as nigh the water as\n",
            "      they possibly can without falling in. And there they stand—miles of\n",
            "      them—leagues. Inlanders all, they come from lanes and alleys,\n",
            "      streets and avenues—north, east, south, and west. Yet here they all\n",
            "      unite. Tell me, does the magnetic virtue of the needles of the compasses\n",
            "      of all those ships attract them thither?\n",
            "    \n",
            "\n",
            "      Once more. Say you are in the country; in some high land of lakes. Take\n",
            "      almost any path you please, and ten to one it carries you down in a dale,\n",
            "      and leaves you there by a pool in the stream. There is magic in it. Let\n",
            "      the most absent-minded of men be plunged in his deepest r\n"
          ]
        }
      ],
      "source": [
        "# Exibir o livro\n",
        "soup = BeautifulSoup(html, 'html.parser')\n",
        "\n",
        "text = soup.get_text()\n",
        "\n",
        "print(text[32000:34000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ8Uk1P_tkDP"
      },
      "source": [
        "# **Pré-Processamento**\n",
        "O resultado de cada uma das etapas a seguir **deve ser utilizada como entrada para as etapas posteriores!!!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SGvDYCC4tng_"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "moby dick; or the whale\n",
            "\n",
            "\n",
            "\n",
            "*** start of the pr\n"
          ]
        }
      ],
      "source": [
        "#Inicialmente normalize o texto, tornando-o completamente minúsculo. Você pode fazer isso usando a função lower() da classe string.\n",
        "def normalizar(texto):\n",
        "    return texto.lower()\n",
        "\n",
        "texto_normalizado = normalizar(text)\n",
        "print(texto_normalizado[0:50])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rLtNJh41yQk3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens: 219426\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'or', 'the', 'whale', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
          ]
        }
      ],
      "source": [
        "# Tokenize o texto considerando apenas palavras que possuem caracteres alpha-numéricos. Você pode fazer isso com expressões regulares e a biblioteca nltk.tokenize.RegexpTokenizer() passando seu padrão ou re.find()\n",
        "def tokenizar(texto):\n",
        "   return RegexpTokenizer(r'(\\w+)').tokenize(texto)\n",
        "\n",
        "tokens = tokenizar(texto_normalizado)\n",
        "\n",
        "print(f\"Número de tokens: {len(tokens)}\")\n",
        "print(\"Primeiros 20 tokens:\", tokens[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CBlxozG5oOsX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens: 210103\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'or', 'the', 'whale', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'or', 'the', 'whale', 'by', 'herman', 'melville']\n"
          ]
        }
      ],
      "source": [
        "# Implemente o código para remover tokens que possuem tamanho maior que 15 e menor que 2. Você pode fazer isso usando a função len() para verificar o tamanho das palavras\n",
        "def remove_palavras_pequenas_grandes(tokens):\n",
        "    return [token for token in tokens if len(token) > 1 and len(token) < 15]\n",
        "\n",
        "\n",
        "process_1 = remove_palavras_pequenas_grandes(tokens)\n",
        "print(f\"Número de tokens: {len(process_1)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_1[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O-b1_sHpoQP2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens: 111121\n",
            "Primeiros 20 tokens: ['moby', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'moby', 'dick', 'whale', 'herman', 'melville', 'contents', 'etymology', 'extracts', 'supplied', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ],
      "source": [
        "# Implemente esta função para ela retornar uma lista de tokens sem stopwords. Você pode fazer isso usando a função stopwords.words('english') para encontrar todas as stopwords\n",
        "def remove_stopwords(tokens):\n",
        "  return [token for token in tokens if token not in stopwords.words('english')]\n",
        "\n",
        "\n",
        "process_2 = remove_stopwords(process_1)\n",
        "print(f\"Número de tokens: {len(process_2)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_2[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "oclfKIqZplCp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens: 111121\n",
            "Primeiros 20 tokens: ['mobi', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'mobi', 'dick', 'whale', 'herman', 'melvil', 'content', 'etymolog', 'extract', 'suppli', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ],
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens stemmizados. Você pode fazer isso usando a função stem() da classe PorterStemmer e passando para ela a palavra desejada\n",
        "def stemming(tokens):\n",
        "  return [PorterStemmer().stem(token) for token in tokens]\n",
        "\n",
        "\n",
        "process_3 = stemming(process_2)\n",
        "print(f\"Número de tokens: {len(process_3)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_3[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ac7r3rursJqR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número de tokens: 111121\n",
            "Primeiros 20 tokens: ['mobi', 'dick', 'whale', 'start', 'project', 'gutenberg', 'ebook', '2701', 'mobi', 'dick', 'whale', 'herman', 'melvil', 'content', 'etymolog', 'extract', 'suppli', 'sub', 'sub', 'librarian']\n"
          ]
        }
      ],
      "source": [
        "# Implemente esta função para ela retornar uma lista com os tokens lematizados. Você pode fazer isso usando a função lemmatize() da classe WordNetLemmatizer e passando para ela a palavra desejada\n",
        "def lemmatize(texto):\n",
        "  return [WordNetLemmatizer().lemmatize(token) for token in texto]\n",
        "\n",
        "\n",
        "process_4 = lemmatize(process_3)\n",
        "print(f\"Número de tokens: {len(process_4)}\")\n",
        "print(\"Primeiros 20 tokens:\", process_4[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw7dCTUQBZJW"
      },
      "source": [
        "## **Frequência dos tokens**\n",
        "Mostre a distribuição das palavras para cada uma das etapas de pré-processamento realizadas. Você pode fazer isso utilizando a função nltk.FreqDist() passando sua lista de palavras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "dMq3DqFP71f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'the': 14538, 'of': 6626, 'and': 6447, 'to': 4627, 'in': 4184, 'that': 3085, 'his': 2532, 'it': 2522, 'he': 1897, 'but': 1818, ...})"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Frequência das palavras depois de normalizadas e das pequenas serem removidas\n",
        "nltk.FreqDist(process_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4atACCWT76xF"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'the': 14538, 'of': 6626, 'and': 6447, 'a': 4747, 'to': 4627, 'in': 4184, 'that': 3085, 'his': 2532, 'it': 2522, 'i': 2127, ...})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Frequência das palavras tokenizadas\n",
        "nltk.FreqDist(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Qf6_YOps78hI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'whale': 1646, 'one': 943, 'like': 661, 'ship': 625, 'upon': 566, 'ye': 548, 'sea': 542, 'man': 541, 'ahab': 518, 'boat': 484, ...})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Frequência das palavras tokenizadas e com stemming\n",
        "nltk.FreqDist(process_3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eY8j-7P28AWG"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FreqDist({'whale': 1646, 'one': 943, 'like': 661, 'ship': 625, 'upon': 566, 'ye': 548, 'sea': 542, 'man': 541, 'ahab': 518, 'boat': 484, ...})"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Frequência das palavras tokenizadas, com stemming e lemming\n",
        "nltk.FreqDist(process_4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngWYeRR8ybtb"
      },
      "source": [
        "# Perguntas\n",
        "\n",
        "1. **Compare o texto original com o texto após a normalização. Quais diferenças você percebe nas palavras e na estrutura geral?**\n",
        "- Resposta: `aqui`\n",
        "\n",
        "2. **O que muda na quantidade de tokens ao aplicar a função remove_palavras_pequenas_grandes()? Informe abaixo o total de tokens antes e depois da aplicação da função.**\n",
        "- Resposta: `aqui`\n",
        "\n",
        "3. **Existe diferença entre o pré-processamento antes e depois de aplicar lematização e stemming?**\n",
        "- Resposta: `aqui`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V71AONUTvHsq"
      },
      "source": [
        "# **Algoritmo Byte-Pair Encoding:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEhXHfgEvOAv"
      },
      "source": [
        "Considerando as três palavras mais frequentes resultantes da lemmatização (process_4), execute, à mão, o algoritmo BPE para k = 3 e insira abaixo o vocabulário e regras obtidas:\n",
        "\n",
        "\n",
        "\n",
        "*   vocabulário obtido: `vocabulário aqui`\n",
        "*   regras obtidas: `regras aqui`\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL7AUQkc_qau"
      },
      "source": [
        "# **Distância de Edição**\n",
        "Calcule à mão a distância de edição entre as palavras 'while' e 'like' supondo que inserções ou deleções tem custo 1 e uma substituição tem custo 2 quando os caracteres são diferentes e custo 0 quando são iguais.\n",
        "\n",
        "\n",
        "Insira o valor obtido aqui: `5`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6UITqLZzKKx"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
